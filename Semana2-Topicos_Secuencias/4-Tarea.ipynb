{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea - Semanal\n",
    "\n",
    "## Dataset: Correos de Enron\n",
    "\n",
    "Enron fue una empresa petrolera norteamericana que quebró estrepitosamente en el año 2001 luego de haber hecho lo que fue hasta su momento el fraude contable más grande de la historia. Como parte de la investigación criminal, se analizaron cientos de miles de mensajes de los servidores de correo electrónico de la compañía. \n",
    "\n",
    "El dataset está disponible para el público desde hace varios años. Aquí tenemos una muestra preprocesada con aproximadamente 10,000 mensajes de correo electrónico listos para ser analizados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos bibliotecas necesarias para la tarea\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Leemos el dataset\n",
    "enron_file = open('resources/enron.csv', 'r') \n",
    "enron = enron_file.readlines()\n",
    "enron = [ l.rstrip() for l in enron ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO 1 (1 punto)\n",
    "### - CONSTRUIR UN VECTORIZADOR DE CONTEOS QUE ELIMINE STOP-WORDS EN INGLÉS PARA EL DATASET enron\n",
    "### - UTILIZARLO PARA TRANSFORMAR EL DATASET EN UNA MATRIZ DOCUMENTO-PALABA\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z][a-zA-Z]+\\b\").fit(enron)\n",
    "\n",
    "### TODO: Crear el count vectorizer y el fit.\n",
    "enron_matrix = cv.transform(enron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "*** TOPICO  0\n",
      "['market' 'iso' 'power' 'energy' 'board' 'price' 'gas' 'california' 'ufe'\n",
      " 'prices' 'said' 'electricity' 'trade' 'cap' 'state' 'san' 'data' 'new'\n",
      " 'years' 'rates' 'think' 'going' 'high' 'just' 'project' 'dont' 'curve'\n",
      " 'bid' 'time' 'week']\n",
      "\n",
      "\n",
      "*** TOPICO  1\n",
      "['week' 'play' 'sore' 'expected' 'start' 'saturday' 'phillip' 'update'\n",
      " 'lucy' 'questionable' 'practiced' 'practice' 'missed' 'like' 'rent'\n",
      " 'rentroll' 'likely' 'email' 'need' 'wednesday' 'sunday' 'did' 'news'\n",
      " 'friday' 'paid' 'season' 'thursday' 'going' 'time' 'new']\n",
      "\n",
      "\n",
      "*** TOPICO  2\n",
      "['game' 'like' 'texas' 'brown' 'team' 'hes' 'players' 'good' 'said' 'week'\n",
      " 'moving' 'think' 'season' 'just' 'yards' 'year' 'bowl' 'make' 'ball'\n",
      " 'sunday' 'play' 'time' 'dont' 'big' 'simms' 'great' 'better' 'football'\n",
      " 'teams' 'know']\n",
      "\n",
      "\n",
      "*** TOPICO  3\n",
      "['market' 'iso' 'message' 'meeting' 'august' 'california' 'participants'\n",
      " 'information' 'toute' 'client' 'relations' 'ferc' 'price' 'conference'\n",
      " 'mail' 'time' 'simulation' 'account' 'order' 'energy' 'know' 'attached'\n",
      " 'questions' 'number' 'regarding' 'notice' 'section' 'plan' 'stock'\n",
      " 'enron']\n",
      "\n",
      "\n",
      "*** TOPICO  4\n",
      "['eric' 'subject' 'basshouectect' 'know' 'enron' 'let' 'bass' 'america'\n",
      " 'north' 'bryan' 'corp' 'shanna' 'brian' 'going' 'game' 'tonight'\n",
      " 'matthew' 'david' 'want' 'hullhouectect' 'timothy' 'lenharthouectect'\n",
      " 'just' 'think' 'like' 'communications' 'luis' 'make' 'oneal'\n",
      " 'communicationsenron']\n",
      "\n",
      "\n",
      "*** TOPICO  5\n",
      "['new' 'thanks' 'center' 'day' 'mail' 'gas' 'product' 'daily' 'enron'\n",
      " 'eric' 'help' 'message' 'email' 'file' 'houston' 'contact' 'information'\n",
      " 'need' 'products' 'options' 'index' 'points' 'password' 'training'\n",
      " 'meter' 'subject' 'type' 'desk' 'resolution' 'attachment']\n",
      "\n",
      "\n",
      "*** TOPICO  6\n",
      "['image' 'mail' 'travel' 'deal' 'night' 'person' 'hotel' 'package' 'dont'\n",
      " 'cruise' 'starting' 'need' 'great' 'bmc' 'rates' 'starts' 'make'\n",
      " 'includes' 'nights' 'trip' 'accommodations' 'deals' 'specially' 'new'\n",
      " 'acquired' 'thanks' 'heres' 'inclusive' 'visit' 'day']\n",
      "\n",
      "\n",
      "*** TOPICO  7\n",
      "['offset' 'spread' 'trade' 'eol' 'deal' 'value' 'like' 'gas' 'transaction'\n",
      " 'time' 'trades' 'market' 'buys' 'minimum' 'sell' 'buy' 'new' 'just'\n",
      " 'enron' 'possible' 'prices' 'intensity' 'average' 'trading' 'know'\n",
      " 'product' 'sells' 'program' 'think' 'year']\n",
      "\n",
      "\n",
      "*** TOPICO  8\n",
      "['week' 'play' 'value' 'sore' 'start' 'accrual' 'smith' 'expected'\n",
      " 'injury' 'questionable' 'stl' 'listed' 'practice' 'likely' 'car'\n",
      " 'johnson' 'knee' 'game' 'bid' 'nyj' 'report' 'den' 'look' 'total' 'bal'\n",
      " 'ind' 'sunday' 'wednesday' 'williams' 'like']\n",
      "\n",
      "\n",
      "*** TOPICO  9\n",
      "['enron' 'phillip' 'services' 'business' 'meeting' 'know' 'ebs' 'thanks'\n",
      " 'like' 'project' 'need' 'work' 'plan' 'questions' 'let' 'cost'\n",
      " 'development' 'time' 'forward' 'management' 'new' 'jeff' 'communications'\n",
      " 'date' 'strategic' 'relationship' 'agreement' 'jennifer' 'fax' 'attached']\n",
      "\n",
      "\n",
      "*** TOPICO  10\n",
      "['john' 'meeting' 'susan' 'subject' 'sarah' 'houston' 'thanks' 'day'\n",
      " 'enron' 'phone' 'office' 'need' 'friday' 'week' 'time' 'december' 'know'\n",
      " 'desk' 'message' 'bailey' 'work' 'business' 'tuesday' 'monday' 'mail'\n",
      " 'wednesday' 'arnoldhouectect' 'meetings' 'january' 'today']\n",
      "\n",
      "\n",
      "*** TOPICO  11\n",
      "['john' 'subject' 'arnoldhouectect' 'trading' 'houston' 'thanks' 'kim'\n",
      " 'gas' 'ina' 'ward' 'mike' 'ena' 'arnold' 'july' 'group' 'nymex' 'var'\n",
      " 'andy' 'frank' 'know' 'like' 'need' 'phillip' 'ews' 'make' 'traders'\n",
      " 'eol' 'time' 'just' 'liz']\n",
      "\n",
      "\n",
      "*** TOPICO  12\n",
      "['enron' 'said' 'company' 'power' 'energy' 'new' 'business' 'gas' 'year'\n",
      " 'markets' 'jones' 'enrons' 'dow' 'trading' 'million' 'market' 'global'\n",
      " 'state' 'billion' 'india' 'copyright' 'corp' 'natural' 'companies'\n",
      " 'government' 'reuters' 'service' 'news' 'houston' 'financial']\n",
      "\n",
      "\n",
      "*** TOPICO  13\n",
      "['time' 'outlook' 'information' 'migration' 'day' 'enron' 'net' 'cal'\n",
      " 'bmc' 'thank' 'want' 'meeting' 'following' 'gas' 'date' 'ubs' 'works'\n",
      " 'john' 'regards' 'set' 'price' 'need' 'reply' 'questions' 'help' 'notice'\n",
      " 'dec' 'plan' 'storage' 'use']\n",
      "\n",
      "\n",
      "*** TOPICO  14\n",
      "['subject' 'today' 'john' 'gas' 'think' 'like' 'information' 'day' 'know'\n",
      " 'life' 'new' 'free' 'short' 'dont' 'thanks' 'jennifer' 'time' 'enron'\n",
      " 'change' 'cal' 'deal' 'america' 'year' 'good' 'long' 'market'\n",
      " 'arnoldhouectect' 'telluride' 'right' 'price']\n",
      "\n",
      "\n",
      "*** TOPICO  15\n",
      "['image' 'buy' 'gas' 'downgraded' 'click' 'initiated' 'coverage' 'strong'\n",
      " 'market' 'upgraded' 'email' 'prices' 'free' 'change' 'service' 'natural'\n",
      " 'hold' 'save' 'receive' 'price' 'analysis' 'utilities' 'story'\n",
      " 'earningscom' 'mkt' 'new' 'online' 'neutral' 'info' 'said']\n",
      "\n",
      "\n",
      "*** TOPICO  16\n",
      "['john' 'subject' 'just' 'know' 'time' 'need' 'want' 'thanks'\n",
      " 'arnoldhouectect' 'going' 'hey' 'let' 'think' 'enron' 'margaret' 'good'\n",
      " 'work' 'dont' 'new' 'communications' 'brian' 'got' 'like' 'friday' 'file'\n",
      " 'tomorrow' 'did' 'thank' 'sorry' 'allenenron']\n",
      "\n",
      "\n",
      "*** TOPICO  17\n",
      "['wine' 'eric' 'pick' 'post' 'know' 'tasting' 'gonzalez' 'review' 'want'\n",
      " 'points' 'order' 'texas' 'place' 'notes' 'dont' 'best' 'round' 'make'\n",
      " 'number' 'case' 'like' 'new' 'librarycom' 'sale' 'let' 'end' 'week' 'jan'\n",
      " 'tba' 'winfree']\n",
      "\n",
      "\n",
      "*** TOPICO  18\n",
      "['enron' 'information' 'gas' 'swaps' 'time' 'year' 'market' 'sell' 'power'\n",
      " 'swap' 'attached' 'said' 'make' 'futures' 'energy' 'thanks' 'offer'\n",
      " 'outage' 'higher' 'deal' 'week' 'markets' 'plants' 'contract' 'day' 'new'\n",
      " 'just' 'based' 'natural' 'spreads']\n",
      "\n",
      "\n",
      "*** TOPICO  19\n",
      "['sent' 'original' 'message' 'subject' 'john' 'arnold' 'october' 'monday'\n",
      " 'allen' 'phillip' 'tuesday' 'jennifer' 'november' 'fraser' 'wednesday'\n",
      " 'thursday' 'thanks' 'margaret' 'email' 'mike' 'time' 'file' 'ill' 'thank'\n",
      " 'today' 'june' 'friday' 'want' 'morning' 'tomorrow']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### EJERCICIO 2 (4 puntos)\n",
    "### - ENTRENAR UN MODELO DE ASIGNACION LATENTE DE DIRICHLET SOBRE enron_matrix.\n",
    "### - MOSTRAR LOS TÓPICOS RESULTANTES (las 30 palabras más relevantes por tópico)\n",
    "### - PROBAR CON DISTINTOS NÚMEROS DE TÓPICOS, REPORTAR SUS RESULTADOS. \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "## Entrenamos el modelo LDA (toma un par de minutos)\n",
    "lda = LatentDirichletAllocation(n_components=20, verbose=True).fit(enron_matrix)\n",
    "\n",
    "comp = lda.components_\n",
    "vec = np.array(cv.get_feature_names())\n",
    "\n",
    "### EJERCICIO 2.1 (2 puntos)\n",
    "### Despliegue las distribuciones de los topicos\n",
    "### TODO: Desplegar los conteos de las distribuciones (ver notebook #2)\n",
    "for i in range(0, comp.shape[0]):\n",
    "    print('*** TOPICO ', i)\n",
    "    print(vec[comp[i].argsort()[-30:][::-1]])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO 3 (1 punto)\n",
    "### - CONSTRUIR UN LabelEncoder CON LOS DATOS DE enron Y TRANSFORMAR LOS DATOS\n",
    "###   EL LabelEncoder DEBE SEPARAR LOS MENSAJES PALABRA POR PALABRA\n",
    "### - TRANSFORMAR EN DATASETS CODIFICADOS\n",
    "### - CREAR EL VECTOR DE LONGITUD DE MENSAJES\n",
    "###\n",
    "### *** IMPORTANTE: DADO EL TIEMPO EXCESIVO QUE TOMA ENTRENAR ESTE MODELO, \n",
    "###     SE RECOMIENDA TRABAJAR CON UNA MUESTRA REDUCIDA (p.ej. LOS PRIMEROS X MENSAJES)\n",
    "\n",
    "enron_chars = [ mail.lower().split(' ') for mail in enron[-200:] ]\n",
    "\n",
    "chars_len = [len(y) for y in enron_chars] ### TODO: crear arreglo de longitudes de los enron_chars\n",
    "\n",
    "### TODO: realizar el LabelEncoder fir de enron_chars (Pista buscar enc en Notebook #3)\n",
    "\n",
    "cv_chars = LabelEncoder().fit([y for l in enron_chars for y in l])\n",
    "\n",
    "enron_enc = np.concatenate([cv_chars.transform(y).reshape(-1, 1) for y in enron_chars]).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting a model with 82989 free scalar parameters with only 61870 data points will result in a degenerate solution.\n",
      "         1     -558328.1704             +nan\n",
      "         2     -441037.8675     +117290.3028\n",
      "         3     -441031.7286          +6.1389\n",
      "         4     -441022.4028          +9.3258\n",
      "         5     -441008.6420         +13.7608\n",
      "         6     -440990.7912         +17.8508\n",
      "         7     -440970.4198         +20.3714\n",
      "         8     -440948.3700         +22.0498\n",
      "         9     -440924.2891         +24.0810\n",
      "        10     -440897.6887         +26.6004\n"
     ]
    }
   ],
   "source": [
    "### EJERCICIO 4 (4 puntos)\n",
    "### - ENTRENAR UN MODELO OCULTO DE MARKOV (HMM) SOBRE chars_enc Y chars_len\n",
    "### - GENERAR DIEZ SECUENCIAS ALEATORIAS DE DISTINTAS LONGITUDES\n",
    "### - PROBAR CON DISTINTOS NÚMEROS DE COMPONENTES, REPORTAR SUS RESULTADOS. \n",
    "\n",
    "enron_model = hmm.MultinomialHMM(n_iter=10, n_components=10, verbose=True).fit(enron_enc, chars_len)\n",
    "\n",
    "chars_enc = np.concatenate([cv_chars.transform(y).reshape(-1, 1) for y in enron_chars]).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can look blake and that way hector are practiced to\n"
     ]
    }
   ],
   "source": [
    "# Generar Cadenas aleatorias...\n",
    "print(' '.join(cv_chars.inverse_transform(enron_model.sample(10)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea Extra (no cuenta como entregable semanal):\n",
    "\n",
    "En esta tarea ud va replicar este notebook pero con un texto mas largo y va a buscar entrenar el modelo con HMM para generar frases aleatorias.\n",
    "\n",
    "Para este ejercicio va a a tratar de entrenar el modelo usando el libro de Dracula. https://www.gutenberg.org/files/345/345-0.txt (puede utilizar otros libros de us gusto, mas textos en https://www.gutenberg.org)\n",
    "\n",
    "Este ejercicio requiere que ud realice un limapiado del texto primero para resultados optimos.\n",
    "\n",
    "\n",
    "<img src=\"https://e7.pngegg.com/pngimages/518/88/png-clipart-dracula-from-sesame-street-count-von-count-elmo-sesame-street-characters-count-dracula-vampire-sesame-miscellaneous-purple-thumbnail.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
